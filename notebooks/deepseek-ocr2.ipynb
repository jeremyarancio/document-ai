{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd88d7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from document_ai.utils import pdf2image\n",
    "\n",
    "REPO_DIR = Path().absolute().parent\n",
    "\n",
    "filepath = REPO_DIR / \"data/documents/deepseek-ocr-2.pdf\"\n",
    "output_dir = REPO_DIR / \"data/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2416598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pdf2image.convert_pdf_to_images(filepath)\n",
    "image_path = REPO_DIR / \"data/images/page.png\"\n",
    "images[8].save(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3569ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/document-ai/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR2. This is not supported for all configurations of models and can yield errors.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "model_name = 'deepseek-ai/DeepSeek-OCR-2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True, use_safetensors=True)\n",
    "model = model.eval().cuda().to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e27b7947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/document-ai/.venv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "BASE:  torch.Size([1, 256, 1280])\n",
      "PATCHES:  torch.Size([6, 144, 1280])\n",
      "=====================\n",
      "<|ref|>figure_title<|/ref|><|det|>[[112, 99, 883, 165]]<|/det|>\n",
      "Table 1 | Comprehensive evaluation of document reading on OmniDocBench v1.5. V-token \\( ^{max} \\)  represents the maximum number of visual tokens used per page in this benchmark. R-order denotes reading order. Except for DeepSeek OCR and DeepSeek OCR 2, all other model results in this table are sourced from the OmniDocBench repository.\n",
      "\n",
      "<|ref|>table<|/ref|><|det|>[[119, 175, 872, 594]]<|/det|>\n",
      "<table><tr><td>Model</td><td>\\( |V-\\text{token}^{max}\\downarrow|Overall\\uparrow|Text^{Edit}\\downarrow Formula^{CDM}\\uparrow Table^{TEDs}\\uparrow Table^{TEDs_{s}}\\uparrow R-order^{Edit}\\uparrow \\)</td></tr><tr><td colspan=\"3\">Pipeline</td></tr><tr><td>Marker-1.8.2 [1]</td><td>-</td></tr><tr><td>MinerU2-pp [45]</td><td>-</td></tr><tr><td>Dolphin [17]</td><td>-</td></tr><tr><td>Dolphin-1.5 [17]</td><td>-</td></tr><tr><td>PP-StructureV3 [13]</td><td>-</td></tr><tr><td>MonkeyOCR-pro-1.2B [23]</td><td>-</td></tr><tr><td>MonkeyOCR-3B [23]</td><td>-</td></tr><tr><td>MonkeyOCR-pro-3B [23]</td><td>-</td></tr><tr><td>MinerU2.5 [45]</td><td>-</td></tr><tr><td>PaddleOCR-VL [12]</td><td>-</td></tr><tr><td colspan=\"3\">End-to-end Model</td></tr><tr><td>OCRFlux [4]</td><td>&gt;6000</td></tr><tr><td>GPT-4o [33]</td><td>-</td></tr><tr><td>InternVL3 [55]</td><td>&gt;7000</td></tr><tr><td>POINTS-Reader [31]</td><td>&gt;6000</td></tr><tr><td>olmOCR [36]</td><td>&gt;6000</td></tr><tr><td>InternVL3.5-241B [49]</td><td>&gt;7000</td></tr><tr><td>MinerU2-VLM [45]</td><td>&gt;7000</td></tr><tr><td>Nanonets-OCR-s [2]</td><td>&gt;7000</td></tr><tr><td>Qwen2.5-VL-72B [9]</td><td>&gt;6000</td></tr><tr><td>Gemini-2.5 Pro [6]</td><td>-</td></tr><tr><td>dots.ocr [39]</td><td>&gt;6000</td></tr><tr><td>OCRVerse [3]</td><td>&gt;6000</td></tr><tr><td>Qwen3-VL-235B [8]</td><td>&gt;6000</td></tr><tr><td>DeepSeek-OCR (9-crops)</td><td>1156</td></tr><tr><td>DeepSeek-OCR 2</td><td>1120</td></tr><tr><td></td><td>↓36</td></tr></table>\n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[114, 617, 256, 634]]<|/det|>\n",
      "## 5. Evaluation\n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[112, 648, 884, 731]]<|/det|>\n",
      "We select OmniDocBench v1.5  \\( [34] \\)  as our primary benchmark for evaluation. This benchmark comprises 1,355 document pages spanning 9 major categories (including magazines, academic papers, research reports, and so on) in both Chinese and English. With its diverse test samples and robust evaluation criteria, OmniDocBench provides an effective framework for validating the performance of DeepSeek-OCR 2, particularly the effectiveness of DeepEncoder V2.\n",
      "\n",
      "<|ref|>sub_title<|/ref|><|det|>[[114, 752, 270, 768]]<|/det|>\n",
      "## 5.1. Main Results\n",
      "\n",
      "<|ref|>text<|/ref|><|det|>[[112, 778, 885, 893]]<|/det|>\n",
      "As shown in Table 1, DeepSeek-OCR 2 achieves advanced performance of 91.09% while using the smallest upper limit of visual tokens (V-token \\( ^{max} \\) ). Compared to the DeepSeek-OCR baseline, it demonstrates a 3.73% improvement under similar train data sources, validating the effectiveness of our newly designed architecture. Beyond the overall improvement, the Edit Distance (ED) for reading order (R-order) has also significantly decreased (from 0.085 to 0.057), indicating that the new DeepEncoder V2 can effectively select and arrange initial visual tokens based on image information. As illustrated in Table 2, DeepSeek-OCR 2 (0.100) achieves lower ED in document\n",
      "===============save results:===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "image: 0it [00:00, ?it/s]\n",
      "other: 100%|██████████| 6/6 [00:00<00:00, 84449.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"<image>\\nFree OCR. \"\n",
    "prompt = \"<image>\\n<|grounding|>Convert tables to markdown. \"\n",
    "image_file = str(image_path)\n",
    "\n",
    "res = model.infer(\n",
    "    tokenizer,\n",
    "    prompt=prompt,\n",
    "    image_file=image_file,\n",
    "    output_path=output_dir,\n",
    "    base_size=1024,\n",
    "    image_size=768,\n",
    "    crop_mode=True,\n",
    "    save_results=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5c0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabca0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dad489b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "document-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
